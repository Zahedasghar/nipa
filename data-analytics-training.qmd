---
title: "Data Analytics for Strategic Decision-Making"
subtitle: "45th Mid-Career Management Course (MCMC)"
author: "National Institute of Public Administration, Peshawar"
date: "January 13, 2026"
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
    chalkboard: true
    preview-links: auto
    logo: nipa_logo.jpg
    css: styles.css
    footer: "NIPA Peshawar | MCMC-45 | Data Analytics Workshop"
    transition: slide
    background-transition: fade
    incremental: false
    code-fold: true
    code-tools: true
---

# Welcome {background-color="#2C5F2D"}

## Session Overview

### Learning Objectives:

1. Distinguish data from governance failures
2. Make defensible evidence-based decisions
3. Avoid common analytical errors
4. Lead data governance institutionally


::: {.notes}
Welcome participants. Emphasize this is NOT a technical training - it's about leadership, accountability, and strategic judgment using data.
:::

---

# Part 1: The Mindset Shift {background-color="#2C5F2D"}

## From Technical Executor to Strategic Leader

::: {.columns}
::: {.column width="50%"}
### Earlier Focus 
- Implementing systems
- Following procedures  
- Managing processes
- Technical execution
:::

::: {.column width="50%"}
### Your Responsibility Now 
- **Owning institutional decisions**
- **Defending under audit**
- **Shaping what gets measured**
- **Strategic judgment under uncertainty**
:::
:::

. . .

::: {.callout-important icon=false}
## Core Principle
"Your job is not to trust data blindly or reject it instinctivelyâ€”but to **own decisions** that data informs."
:::

---

## What Is Data Analytics? 

::: {.incremental}
- **Technical Definition:** Systematic use of evidence to inform decisions

- **Your Reality:** Authority, accountability, and institutional risk

- **The Real Questions:**
  - When do I need data for a defensible decision?
  - When should I override data-driven recommendations?
  - How do I defend my choices under audit or political scrutiny?
  - How do I ensure analytics reveals problems rather than conceals them?
:::

---

## Two Types of Failures

::: {.columns}
::: {.column width="50%"}
### Data Failure
**Symptom:** Incomplete, outdated, or contradictory information

**Examples:** 

- Beneficiary databases not updated 

- Indicators measuring compliance not outcomes 

- Siloed data across departments 

- Systems that exist but aren't integrated
:::

::: {.column width="50%"}
### Governance Failure  
**Symptom:** Institutional clarity gaps

**Examples:** 

- Unclear who decides what 

- Officers fear analytics = audit exposure 

- No mandate for cross-departmental action 

- Political pressure overrides evidence 

- Weak accountability mechanisms
:::
:::

. . .

::: {.callout-warning}
**Critical Insight:** Most "data problems" in government are actually **governance problems**
:::

---

# Part 2: Common Errors in Data Analysis {background-color="#2C5F2D"}

## Framework from "Thinking Clearly with Data"

---

## Error 1: Selection Bias

::: {.callout-note icon=false}
## Definition
**Selection bias** occurs when the sample analyzed is not representative of the population, leading to distorted conclusions.
:::

### Pakistan Example: School Dropout Analysis

::: {.columns}
::: {.column width="50%"}
**What the data shows:** 

- Survey of enrolled students shows 95% satisfaction 

- District education officer reports low dropout rates 

- Dashboard shows green indicators
:::

::: {.column width="50%"}
**What's missing:** 

- **Dropped-out students aren't in the sample!** 

- Out-of-school children invisible 

- Selection bias creates false confidence
:::
:::

. . .

**Officer's Question:** "Am I making decisions based on who **stayed** or who **left**?"

---

## Error 2: Confounding Variables

::: {.callout-note icon=false}
## Definition
**Confounding** happens when an unmeasured factor influences both the treatment and outcome, creating spurious relationships.
:::

### Pakistan Example: Health Clinic Performance

::: {.incremental}
- **Observation:** Clinic A has better health outcomes than Clinic B
- **Quick conclusion:** Clinic A's management is superior
- **Confounding reality:**
  - Clinic A is in urban area (better infrastructure)
  - Clinic B serves remote, poorer population
  - Socioeconomic status confounds the relationship
  - Management quality might be identical or opposite!
:::

. . .

**Officer's Question:** "What else could explain this pattern besides my preferred explanation?"

---

## Error 3: Reverse Causality

::: {.callout-note icon=false}
## Definition
**Reverse causality** occurs when we mistake effect for causeâ€”the arrow points the wrong direction.
:::

### Pakistan Example: Police Presence and Crime

::: {.columns}
::: {.column width="50%"}
**Pattern observed:** 

- Areas with more police have higher crime rates 

- Data correlation is clear and strong

**Wrong conclusion:**  

"Police presence causes crime!"
:::

::: {.column width="50%"}
**Actual causality:** 

- High crime areas get more police deployment 

- **Crime causes police presence, not vice versa** 

- Classic reverse causality error

**Correct conclusion:**  

"Police are deployed where needed"
:::
:::

. . .

**Officer's Question:** "Which way does the arrow really point?"

---

## Error 4: Measurement Error & Manipulation

::: {.callout-note icon=false}
## Definition
When indicators become targets, people optimize for metrics rather than underlying goals (**Goodhart's Law**)
:::

### Pakistan Example: Performance Dashboards

::: {.incremental}
 **Initial goal:** Improve service delivery
 **Metric introduced:** "% of files processed within timeline"
 **What happens:**
   - Officers focus on meeting deadlines
   - Complex cases get delayed or simplified
   - Files are "processed" but problems unresolved
   - Dashboard turns green, ground reality unchanged
 **Measurement gaming:** Officers optimize the indicator, not the outcome
:::

. . .

**Officer's Question:** "Are we measuring what matters, or what's easy to measure?"

---

## Error 5: Ecological Fallacy

::: {.callout-note icon=false}
## Definition
**Ecological fallacy** happens when we incorrectly infer individual-level relationships from aggregate data.
:::

### Pakistan Example: District Education Budget

::: {.columns}
::: {.column width="60%"}
**Aggregate observation:**
- District X: High education budget per capita
- District X: Low literacy rates

**Tempting conclusion:**
"Education spending doesn't improve literacy"
:::

::: {.column width="40%"}
**Individual reality:**
- Budget goes to urban elite schools
- Rural children remain underserved
- Aggregate data hides distribution
- Individual outcomes vary dramatically
:::
:::

. . .

**Officer's Question:** "Does the average hide important variation?"

---

## Error 6: Survivorship Bias

::: {.callout-note icon=false}
## Definition
**Survivorship bias** occurs when we only analyze "survivors" and ignore those who dropped out, failed, or left.
:::

### Pakistan Example: Training Program Evaluation

::: {.incremental}
- **Data analyzed:** Survey of program completers
- **Finding:** 90% report skills improvement and career benefit
- **Conclusion:** "Highly successful program!"
- **Fatal flaw:** 
  - 40% of participants dropped out mid-program
  - Dropouts excluded from survey (only survivors counted)
  - Those who found it irrelevant or ineffective are invisible
  - Success rate is artificially inflated
:::

. . .

**Officer's Question:** "Who am I NOT seeing in this data?"

---

## Error 7: P-Hacking & Data Dredging

::: {.callout-note icon=false}
## Definition
**P-hacking** is manipulating analysis until you find a statistically significant result, creating false discoveries.
:::

### Pakistan Example: Policy Impact Evaluation

::: {.incremental}
- **Scenario:** Expensive program shows no clear impact
- **Pressure:** "Find something positive for the report"
- **P-hacking tactics:**
  - Test 20 different outcomes until one shows p<0.05
  - Subdivide data by gender, age, region until something is "significant"
  - Exclude "outliers" that weaken the result
  - Report only the positive finding, hide the rest
- **Result:** Spurious conclusion based on statistical noise
:::

. . .

**Officer's Question:** "Am I finding patterns, or manufacturing them?"

---

## Error 8: Ignoring Base Rates (Base Rate Fallacy)

::: {.callout-note icon=false}
## Definition
**Base rate neglect** occurs when we ignore how common something is in the population when interpreting specific evidence.
:::

### Pakistan Example: Fraud Detection System

::: {.columns}
::: {.column width="50%"}
**System specs:**
- Accuracy: 95% (correctly identifies fraud) 

- False positive: 5% 

- **Sounds excellent!**

**Reality:** 

- Fraud rate: 1% of transactions 

- System flags 100 cases
:::

::: {.column width="50%"}
**What actually happens:**

| | Flagged | Not Flagged |
|---------|---------|-------------|
| **Actual fraud** | 95 | 5 |
| **False positives** | 4,950 | 94,950 |

**Result:** Only 1.9% of flagged cases are actual fraud!

**98% of alerts are false alarms**
:::
:::

. . .

**Officer's Question:** "How rare is what I'm looking for?"

---

## Summary: 8 Critical Errors

| Error | Key Question |
|-------|--------------|
| **Selection Bias** | Am I seeing the full picture or just survivors? |
| **Confounding** | What else could explain this? |
| **Reverse Causality** | Which way does the arrow point? |
| **Measurement Manipulation** | Am I measuring what matters? |
| **Ecological Fallacy** | Does the average hide variation? |
| **Survivorship Bias** | Who am I NOT seeing? |
| **P-Hacking** | Am I finding or manufacturing patterns? |
| **Base Rate Neglect** | How common is this really? |

::: {.callout-tip}
**Your role:** Ask these questions **before** making decisions, not after audit flags problems.
:::

---

# Part 3: Pakistan Case Studies {background-color="#2C5F2D"}

## Real Governance Challenges

---

## Case 1: Targeting Errors in Social Protection

::: {.columns}
::: {.column width="50%"}
### Context
- Federal program uses data to identify beneficiaries
- Sophisticated poverty scoring system
- Complaints increase about exclusion errors

### Problems Identified
- **Data outdated** (last survey 3 years old)
- **Local officers bypass scores** (political pressure)
- **Weak appeals process** (no clear authority)
:::

::: {.column width="50%"}
### Analytical Errors Present
- **Selection bias:** Only current beneficiaries surveyed
- **Measurement error:** Poverty score doesn't capture shocks
- **Reverse causality:** Excluded people become poorer

### BPS-19 Decision Questions
1. âš–ï¸ Who is accountable for targeting errors?
2. ðŸŽ¯ When should discretion override scores?
3. ðŸ“‹ How to defend decisions under audit?
:::
:::

::: {.notes}
Key point: Without institutional clarity, analytics increases contestation rather than solving it.
:::

---

## Case 2: When Dashboards Hide Reality

::: {.columns}
::: {.column width="50%"}
### Context
- Provincial department reports high completion rates
- Dashboard shows green indicators
- Independent evaluation shows limited ground impact

### Problems Identified
- **Indicators measure inputs not outcomes**
- **Reports meet deadlines, not solve problems**
- **Field realities invisible to senior officers**
:::

::: {.column width="50%"}
### Analytical Errors Present
- **Measurement manipulation:** Gaming the metrics
- **Survivorship bias:** Failed cases don't show up
- **Ecological fallacy:** Aggregate hides variation

### Senior Officials Decision Questions
1. ðŸ“Š What indicators should officers demand?
2. ðŸ” How can analytics surface problems?
3. âš ï¸ When to challenge green dashboards?
:::
:::

::: {.notes}
Analytics reinforces bad incentives unless officers reshape what gets measured.
:::

---

## Case 3: Procurement Analytics & Audit Risk

::: {.columns}
::: {.column width="50%"}
### Context
- Internal audit flags price variations
- Data review shows vendor concentration
- Officers fear using analytics

### Problems Identified
- **Data fragmented across departments**
- **Officers fear: analysis = exposure**
- **No clear ownership of analytics**
:::

::: {.column width="50%"}
### Analytical Errors Present
- **Selection bias:** Only audited cases analyzed
- **Confounding:** Price variation may have legitimate reasons
- **P-hacking risk:** Finding problems to justify audit

###  Decision Questions
1. ðŸ›¡ï¸ How can analytics reduce officer risk?
2. ðŸ“œ What safeguards are needed first?
3. ðŸ’¼ How to communicate results to leadership?
:::
:::

::: {.notes}
Without procedural protection, officers avoid analytics despite inefficiencies.
:::

---

## Case 4: Early Warning & Preventive Governance

::: {.columns}
::: {.column width="50%"}
### Context
- Multiple departments collect data
- Individual datasets appear normal
- Combined analysis shows early stress signals
- No one has authority to act

### Problems Identified
- **Data siloed across departments**
- **No cross-departmental mandate**
- **Officers hesitate on soft signals**
:::

::: {.column width="50%"}
### Analytical Errors Present
- **Base rate fallacy:** Rare events hard to detect
- **Ecological fallacy:** Department-level aggregates hide risks
- **Confounding:** Multiple factors create stress

### Decision Questions
1. â° When should incomplete evidence trigger action?
2. â“ How much uncertainty is acceptable?
3. âœ‹ Who takes responsibility for questioned signals?
:::
:::

::: {.notes}
At senior levels, analytics is about timely, defensible judgment under uncertainty.
:::

---

# Part 4: Pakistan's Data Infrastructure {background-color="#2C5F2D"}

## Context You Need to Know

---

## Big Data & AI in Pakistan {.scrollable}

::: {.columns}
::: {.column width="50%"}
### Big Data: The 5 V's
 **Volume:** Scale (terabytes to petabytes) 

 **Velocity:** Speed of generation 

 **Variety:** Different formats (structured/unstructured) 

 **Veracity:** Quality & trust 

 **Value:** Extracting actionable insights
:::

::: {.column width="50%"}
### Pakistan's Data Infrastructure
- Pakistan Bureau of Statistics (PBS)
- Open Data Portal (data.gov.pk)
- NADRA National Database
- Provincial health & education systems
- PIFRA financial management system
:::
:::

### AI Applications in Government
- Fraud detection in tax & benefits
- Predictive service demand modeling
- Document processing automation
- Citizen service chatbots

::: {.callout-warning}
**Governance Challenge:** These systems existâ€”but data remains siloed. Your role: create institutional mechanisms for cross-departmental analytics while protecting officer discretion.
:::

---

## ERP Systems & Data Governance {.scrollable}

::: {.columns}
::: {.column width="50%"}
### Federal Government Systems
- **PIFRA:** Integrated financial reporting
- **HRMIS:** Employee records & payroll
- **PPRA Portal:** Procurement regulation
- **Budget System:** Planning & allocation

### Benefits of Integration
âœ“ Single source of truth  
âœ“ Reduced duplication  
âœ“ Real-time reporting  
âœ“ Better audit trails  
:::

::: {.column width="50%"}
### Data Governance Principles

1. **Quality:** Accurate, complete, timely
2. **Accessibility:** Right people, right time
3. **Accountability:** Clear ownership
4. **Security:** Protection with appropriate access

### Your Accountability as Senior Officer

You define: 

- Which data justifies which decisions 

- Who has access to what information 

- How to defend choices under audit 


**Systems enable decisions; officers own them**
:::
:::

---

# Part 5: Practical Exercise {background-color="#2C5F2D"} 

## Decision Simulation 

---

## Exercise Structure

::: {.panel-tabset}

### Part 1: Case Analysis 
**Your Task:**
- Form groups of 4-5 officers
- Select one of the four cases presented
- Identify the real problem: Data failure or governance failure?
- Which analytical errors are present?

### Part 2: Decision Framework 
**Develop:**
1. Your decision as the accountable officer
2. How to defend under audit
3. Institutional safeguards needed
4. What data governance changes you'd recommend

### Part 3: Presentations 
**Present to:**
- Audit panel (scrutiny role)
- Political leadership (accountability role)
- Media (transparency role)

**Evaluation:** Not technical correctnessâ€”but clarity of authority, defensibility of judgment, institutional awareness

:::

---

## Group Exercise Materials

You will be provided:

1. **Detailed case file** with background, stakeholders, timeline
2. **Data samples** (some reliable, some questionable)
3. **Institutional constraints** (rules, political pressures, audit requirements)
4. **Decision template** to structure your response

::: {.callout-important}
## Remember
This is not an academic exerciseâ€”it simulates real institutional accountability you'll face at BPS-18/19 level.
:::

---

# Part 6: Decision-Making Framework {background-color="#2C5F2D"}

## Putting It All Together

---

## Your Four Core Responsibilities

::: {.incremental}
### 1. Establish Institutional Clarity
- Define who decides what
- Document decision-making authority
- Create clear escalation paths
- Separate technical analysis from policy judgment

### 2. Create Officer Safeguards
- Reduce exposure while improving transparency
- Document decision rationale contemporaneously
- Build peer review mechanisms
- Establish audit-defensible processes
:::

---

## Your Four Core Responsibilities (continued)

::: {.incremental}
### 3. Reshape Incentives
- Make analytics surface problems, not conceal them
- Reward officers who identify issues early
- Protect those who challenge green dashboards
- Measure outcomes, not just compliance

### 4. Own Decisions Informed by Data
- Accept uncertainty as normal
- Use data to reduce uncertainty, not eliminate it
- Defend judgment under political/audit scrutiny
- Balance evidence with institutional wisdom
:::

---

## Decision Quality Framework {.scrollable}

When facing a data-driven decision, ask:

::: {.incremental}
1. **Clarity:** Do I have authority to make this decision?
2. **Evidence:** What data do I have, and what are its limitations?
3. **Errors:** Which analytical errors might be present?
4. **Alternatives:** What other explanations exist?
5. **Uncertainty:** What don't I know, and does it matter?
6. **Safeguards:** How do I document this for future audit?
7. **Accountability:** Can I defend this under scrutiny?
8. **Learning:** What would tell me I was wrong?
:::

---

## Common Patterns Across All Cases

::: {.columns}
::: {.column width="50%"}
### Data Failures 

- Incomplete or outdated information 

- Siloed across departments 

- Indicators measure compliance, not outcomes 

- Systems exist but aren't integrated

### Solutions 

âœ“ Invest in data quality  
âœ“ Create data-sharing protocols  
âœ“ Redesign metrics  
âœ“ Mandate system integration  
:::

::: {.column width="50%"}
### Governance Failures 

- Unclear lines of authority 

- Officers fear analytics = audit exposure 

- Political pressure overrides evidence 

- No mandate for cross-departmental action

### Solutions 

âœ“ Document decision authorities  
âœ“ Create officer protections  
âœ“ Strengthen institutional independence  
âœ“ Establish cross-cutting mandates  
:::
:::

---

# Conclusion {background-color="#2C5F2D"}

## Data for Insight, Not Abdication

---

## Key Principles to Remember

::: {.columns}
::: {.column width="50%"}
### What Data Is
âœ“ One input to judgment  
âœ“ Uncertainty reducer  
âœ“ Conversation starter  
âœ“ Accountability enhancer  

### What Data Is Not
âœ— The decision-maker  
âœ— Perfect or complete  
âœ— A shield from responsibility  
âœ— A substitute for wisdom  
:::

::: {.column width="50%"}
### Your Role at BPS-18/19
- **Data is not authorityâ€”you are**
- **Analytics is not defenseâ€”judgment is**
- **Systems don't decideâ€”officers do**
- **Uncertainty is not weakness**
:::
:::

. . .

::: {.callout-important icon=false}
## Core Message
"Your job is not to trust data blindly or reject it instinctivelyâ€”but to **own decisions** that data informs."
:::

---

## Your Next Actions

::: {.incremental}
### Immediate (This Week)
1. âœ… Identify one analytics opportunity in your domain
2. âœ… Map accountability gaps in your institution
3. âœ… Connect with your data/IT officers

### Short-term (This Month)
4. âœ… Review one dashboardâ€”ask the 8 error questions
5. âœ… Document one decision rationale for future audit
6. âœ… Initiate one cross-departmental data conversation

### Strategic (This Quarter)
7. âœ… Propose one governance reform in your department
8. âœ… Mentor junior officers on data + judgment
9. âœ… Build institutional safeguards for evidence-based decisions
:::

---

## Resources for Further Learning

### Books
- **Thinking Clearly with Data** (Bueno de Mesquita & Fowler) - Core textbook
- **The Data Detective** (Tim Harford) - Accessible introduction
- **Weapons of Math Destruction** (Cathy O'Neil) - Risks of algorithms

### Pakistan Context
- Pakistan Bureau of Statistics reports
- Planning Commission data portal
- Provincial open data initiatives
- Academic research on Pakistan's data systems

### Skills Development
- Data literacy courses (PBS, NIPA)
- GIS and mapping tools
- Statistical thinking workshops
- Open data advocacy

---

## Questions & Discussion {background-color="#2C5F2D"}

::: {.r-fit-text}
Data and expert judgment matter together
:::

### Contact Information
**Workshop Facilitator:** [Your Name]  
**Email:** rehmatwalikhan@yahoo.com  
**Phone:** 0333-5983705

---

## Thank You {background-color="#2C5F2D"}

::: {.r-fit-text}
From Technical Understanding  
to Institutional Leadership
:::

**Remember:** You own the decisions that data informs.

